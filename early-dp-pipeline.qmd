---
title: "Differential Privacy in a Clinical Setting"
author: "Ahmet Akkoc"
format: pptx
editor: visual
jupyter: ir
---

## **Outline**

-   Patient Data Confidentiality: Dilemma or Opportunity?

-   Differential Privacy: The Great Arbiter

-   Differential Privacy in a Clinical Setting: Pros and Cons

## Patient Data Confidentiality

**Patient data confidentiality** is the ethical and legal obligation to protect patient information.

-   This data includes **medical records**, **test results**, and **personal information**

-   It is a continuous process involving **access control authorization** and **encryption**.

## **Risks of Having Weak Patient Data Confidentiality**

-   **Identity theft** and **fraud**

-   **Privacy violations**

-   **Financial losses**

-   **Potentially illegal**

Clearly, we always want to have some level of confidentiality. But...

## **Limits of a Closed Research Environment**

**Data Confidentiality** puts the researcher into a **closed research environment**. A **closed environment** brings several serious caveats.

-   **Limited access** to resources and data

-   **Restricted collaboration** with external experts

-   **Difficult to replicate** results

    ![](img/researcher.jpg){fig-align="right"}

## **Limits of a Closed Research Environment**

Our own Bente Glintborg is working on a narrative paper highly relevant to closed-research environment limitations:

-   Legal and data-protection obstacles jeopardize patient interests in personalised medicine's research: Experiences from a Nordic collaborative study within rheumatoid arthritis

## **What is Differential Privacy**

Differential Privacy means to add a level of uncertainty to the data, especially sensitive data. The goal is to make identifying this sensitive information more difficult.

![](img/DP.png)

DP is usually accomplished by taking the original values in a dataset and adding random noise. Every patient gains plausible deniability on their data, but aggregate results will remain relatively unchanged.

## **Who devoloped with Differential Privacy?**

DP is largely due to the work of American computer scientist/mathematician Cynthia Dwork. As the culmination of decades of work, it was formalized in the 2010s.

![](img/Cynthia_Dwork.jpeg){fig-align="center"}

A big influence on Dwork's work actually came from Sweden. Namely, mathematician and data privacy pioneer Tore Dalenius who was at some point also at Uppsala.

![](img/Tore_Dalenius.jpg)

## **Differential Privacy in Our Lives**

Google uses differential privacy in its Google Maps service. When users of Google Maps search for a location or request directions, the service collects data about their location and the location they are searching for. DP perturbs the reported location to make sure it isn't the user's true location.

<!--# (Also: keyword suggestion on Android Keyboard, smart-highlighting words on top-left says "Map", "Define" etc. with DP and FL) -->

![](img/map-example.png){fig-align="center"}

This helps to ensure that the privacy of individual data points is protected, while still allowing the location data to be used for statistical analysis and machine-learning.

## Let's Take an Example Patient Dataset

```{r}
# Libraries
library(diffpriv) #diffpriv
library(tidyverse) #Standard for Data Science
library(tidymodels) #tidyverse extension
library(survival) #survival analysis
library(dplyr)
library(tidyr)

# Loading Data
load('data/sample.rdata')
DF_Biobank <- as_tibble(johan_dt_relevant_biobank_anon)

print(
paste('Patients in Dataset: ',dim(DF_Biobank)[1],'Columns per patient: ',dim(DF_Biobank)[2]))

```

```{r}
# Preprocessing

# FILL IN NAs
DF_Biobank[DF_Biobank==""]<-NA

# 

# Column Selection

# Too many columns,
# select the ones we want

baselineCovariates <- c('antiCCP', 'current_smoker_latest',
       'sjc28_m0', 'tjc28_m0','pga_m0', 'crp_m0','ega_m0','pain_m0',
        'sex','erosive_status_baseline','haq_m0','fatigue_m0',
        'age',
        'Igm_rf')

    
targetOutcomes <- c(
  #'sdai_remission_m0',
  #'sdai_remission_m3',
  #'sdai_remission_m6broad',
  #'sdai_remission_m12'
  'sdai_m0',
  'sdai_m3',
  'sdai_m6',
  'sdai_m12'
)

convert_to_boolean <- targetOutcomes
convert_to_double <- c('age','sjc28_m0','tjc28_m0','pga_m0','crp_m0','ega_m0','pain_m0','haq_m0','fatigue_m0',
  'sdai_m0',
  'sdai_m3',
  'sdai_m6',
  'sdai_m12')
convert_to_int <- c()
convert_to_factor <- c('antiCCP','current_smoker_latest',
                       'sex','erosive_status_baseline',
                       'Igm_rf')



neededColumns <- union(baselineCovariates, targetOutcomes)

# Type Conversion
DF_Biobank[convert_to_double] <- lapply(DF_Biobank[convert_to_double], as.double)
DF_Biobank[convert_to_factor] <- lapply(DF_Biobank[convert_to_factor], as.factor)
DF_Biobank[convert_to_int] <- lapply(DF_Biobank[convert_to_int], as.integer)

#Some implementations prefer factors to logicals
DF_Biobank[convert_to_boolean] <- lapply(DF_Biobank[convert_to_boolean], as.factor)

print('Some patient attributes:')
print(neededColumns)

#print('Potentially age is sensitive:')
```

## In this Dataset, Age is Potentially Sensitive

```{r}
head(DF_Biobank['age'])
```

```{R}
head(DF_Biobank['age'])
# In this dataset, age is stored as a real number (with digits)
# And can be reverse-engineered to find the patient's birthday

# Many celebrity birthdays are well-known
# F.x. Alicia Vikander born 3 October, 1988
# is 34,3123 years old, or 12 524 days.
```

![](img/vikander_ex_machina.png)

## Differential Privacy: Under the Surface

```{r}
f <- function(X) X$age  ## identity

D <- (head(DF_Biobank)) ##

n<-nrow(D)  ## dataset size

mechanism <- DPMechLaplace(target = f, sensitivity = 80/n, dims = 1)
pparams <- DPParamsEps(epsilon = 1.25)

r <- releaseResponse(mechanism, privacyParams = pparams, X = D)

cat("DP perturbed data:", r$response,
"\nOriginal: ", f(D))

VISDAT <- data.frame(
  id <- 1:nrow(D),
  original <- f(D),
  privatized <- r$response
)

#print(VISDAT)

#ggplot(VISDAT, aes(truth, y = ..density..,fill="truth")) + 
#geom_histogram(alpha = 0.5,  position = 'identity')#+ 
#geom_histogram(data=VISDAT, aes(fill="priv"), 
#                 alpha = 0.5, position = 'identity')+xlim(0,12)

#par(mfrow=c(1,2)) 
#plot(VISDAT$truth)
#plot(VISDAT$priv)

ggplot(data = VISDAT, aes(x = id)) +
  geom_point(aes(color='Data before',y=original)) +
  #geom_segment( aes(color='Data before',x=id, xend=id, y=original, yend=original))+
  geom_point(aes(color='DP-altered data',y=privatized)) + 
  geom_segment( aes(color='DP-altered data',x=id, xend=id, y=original, yend=privatized))+
  ylab("Age") + xlab("Patient ID") + theme_bw()
```

## Differential Privacy in Action

```{R}
f <- function(X) mean(X$age,na.rm=TRUE) ##  STEP 1: f is our target function (mean age in the dataset)
```

```{r}
f <- function(X) mean(X$age,na.rm=TRUE) ## f is our target function (mean age in the dataset)
```

## Differential Privacy in Action

```{R}

D <- (DF_Biobank) ## STEP 2: We define our confidential dataset as D
n <- nrow(D)      ## Take the patient count

mechanism <- DPMechLaplace(target = f, sensitivity = 80/n, dims = 1)
## STEP 3: 
## We need to pick a noise mechanism based on sensitive attribute (age) in the dataset
## Age is continuous (decimal precision) so we use add Laplace noise

pparams <- DPParamsEps(epsilon = 1.25)
## STEP 4: 
## epsilon is a constant we use to make the resulting data more probable.
## Higher epsilon means more accuracy, but lower privacy.
## Lower epsilon means less accuracy, but higher privacy.
```

```{r}

D <- (DF_Biobank) ## We define our confidential dataset as D
n <- nrow(D)      ## Patient count

mechanism <- DPMechLaplace(target = f, sensitivity = 80/n, dims = 1)
## We need to pick a noise mechanism based on sensitive attribute (age) in the dataset
## Age is continuous (decimal precision) so we use add Laplace noise

pparams <- DPParamsEps(epsilon = 1)
## epsilon is a constant we use to make the resulting data more probable.
## Higher epsilon means more precision, but lower privacy.
## Lower epsilon means less precision, but higher privacy.
```

## Differential Privacy in Action

```{R}
r <- releaseResponse(mechanism, privacyParams = pparams, X = D)
## STEP 5: We apply the DP mechanism with the given params to dataset D

cat("DP response r$response:", r$response,
"\nTrue response f(D): ", f(D))
## Here, you can see a comparison of the DP response and the real f(D), mean age.

```

```{r}
r <- releaseResponse(mechanism, privacyParams = pparams, X = D)

cat("DP response r$response:", r$response,
"\nTrue response f(D): ", f(D))
```

## Differential Privacy in Action (SKIP)

```{r}
f <- function(X) c(mean(X$age,na.rm=TRUE),mean(X$crp_m0,na.rm=TRUE)) ## target function

D <- (DF_Biobank)

# NOTE! Age sensitivity would require knowledge of maximum and minimum age. OR using a hypothetical range? Overestimating the range will lead to a higher noise ratio. Think like YouTube videos with low views, sensitivity can drop as more data is collected.

# NOTE! Also be more careful with CRP because the range is definitely more than (0,1) OR using a hypothetical range? 

n<-nrow(DF_Biobank)  ## dataset size
n
#n <- 100

mechanism <- DPMechLaplace(target = f, sensitivity = 80/n, dims = 1)
#D <- runif(n, min = 0, max = 1) ## the sensitive database in [0,1]^n
pparams <- DPParamsEps(epsilon = 1) ## desired privacy budget

# TODO: How do we tune epsilon? Consider your utility constraints (i.e. bias).

r <- releaseResponse(mechanism, privacyParams = pparams, X = D)
cat("Private response r$response:", r$response,
"\nNon-private response f(D): ", f(D))
```

## Differential Privacy at different levels

```{r}
f <- function(X) X$age  ## identity

D <- (head(DF_Biobank,n=10)  ) ##
#D <- DF_Biobank ##

n<-nrow(D)  ## dataset size

mechanism <- DPMechLaplace(target = f, sensitivity = 80/n, dims = 1)
pparams1 <- DPParamsEps(epsilon = 1.12)
pparams2 <- DPParamsEps(epsilon = 1.25)
pparams3 <- DPParamsEps(epsilon = 3)

r1 <- releaseResponse(mechanism, privacyParams = pparams1, X = D)
r2 <- releaseResponse(mechanism, privacyParams = pparams2, X = D)
r3 <- releaseResponse(mechanism, privacyParams = pparams3, X = D)

#cat("Private response r$response:", r$response,
#"\nNon-private response f(D): ", f(D))
VISDAT <- data.frame(
  id = as.integer(1:nrow(D)),
  truth <- f(D),
  far <- r1$response,
  mid <- r2$response,
  close <- r3$response
)

#print(VISDAT)

#ggplot(VISDAT, aes(truth, y = ..density..,fill="truth")) + 
#geom_histogram(alpha = 0.5,  position = 'identity')#+ 
#geom_histogram(data=VISDAT, aes(fill="priv"), 
#                 alpha = 0.5, position = 'identity')+xlim(0,12)

#par(mfrow=c(1,4)) 
#plot(VISDAT$far)
#plot(VISDAT$mid)
#plot(VISDAT$close)
#plot(VISDAT$truth)

ggplot(data = VISDAT, aes(x = id)) +
  geom_point(aes(color='Without DP',y=truth), size=4, shape=23) +
  #geom_segment( aes(color='Without DP',x=id, xend=id, y=0, yend=truth))+
  geom_point(aes(color='DP (epsilon = 100)',y=close)) +
  geom_segment( aes(color='DP (epsilon = 100)',x=id, xend=id, y=truth, yend=close))+
  geom_point(aes(color='DP (epsilon = 1.25)',y=mid)) + 
  geom_segment( aes(color='DP (epsilon = 1.25)',x=id, xend=id, y=truth, yend=mid))+
  geom_point(aes(color='DP (epsilon = 1.12)',y=far)) +
  geom_segment( aes(color='DP (epsilon = 1.12)',x=id, xend=id, y=truth, yend=far))+
  ylab("Age") + xlab("Patient ID") + theme_bw()
```

## SKIP

``` {#{r}
#library(psych)
#pairs.panels(D)
#names(D)
```

## Can we fit a Regression on DP data?

```{r}
f <- function(X) X$age  ## identity

#D <- (head(DF_Biobank,n=10)  ) ##
D <- DF_Biobank ##
D <- select(D, neededColumns)
D <- na.omit(D)
D <- head(D,n=100)

n<-nrow(D)  ## dataset size

mechanism <- DPMechLaplace(target = f, sensitivity = 80/n, dims = 1)
pparams1 <- DPParamsEps(epsilon = 1.12)
pparams2 <- DPParamsEps(epsilon = 1.25)
pparams3 <- DPParamsEps(epsilon = 3)

r1 <- releaseResponse(mechanism, privacyParams = pparams1, X = D)
r2 <- releaseResponse(mechanism, privacyParams = pparams2, X = D)
r3 <- releaseResponse(mechanism, privacyParams = pparams3, X = D)

#cat("Private response r$response:", r$response,
#"\nNon-private response f(D): ", f(D))

a <- data.frame(
  id = 1:nrow(D),
  label = "True Age",
  age = f(D),
  #far <- r1$response,
  #mid <- r2$response,
  #close <- r3$response,
  outcome = as.double(D$fatigue_m0)
  )

b <- data.frame(
  id = 1:nrow(D),
  label = "with DP (epsilon=1.25)",
  #far <- r1$response,
  age = r2$response,
  #close <- r3$response,
  outcome = as.double(D$fatigue_m0)
  )



#c <- data.frame(
#  id = 1:nrow(D),
#  label = "with DP (epsilon=1.12)",
#  age = r1$response,
  #age = r2$response,
  #close <- r3$response,
#  outcome = as.double(D$sdai_m12)
#  )

VISDAT <- rbind.data.frame(a,b) 
#print(head(VISDAT))

#aes('Without DP',x=outcome, y=truth)

ggplot(VISDAT, aes(x=age, y=outcome, color=label)) +
  geom_point() + 
  geom_smooth(method=lm) +
  ylab("SDAI") + xlab("age") + theme_bw()

print(paste0('Correlation between age and SDAI m12:',cor(x=a$age, y=a$outcome)))
print(paste0('Correlation between age  (DP with epsilon=1.25) and SDAI m12:',cor(x=b$age, y=b$outcome)))
#print(cor(x=c$age, y=c$outcome))

#VISDAT$outcome

```

## Can we fit a Regression on DP data?

```{r}
f <- function(X) X$pga_m0  ## identity

#D <- (head(DF_Biobank,n=10)  ) ##
D <- DF_Biobank ##
D <- select(D, neededColumns)
D <- na.omit(D)
D <- head(D,n=100)

n<-nrow(D)  ## dataset size

mechanism <- DPMechLaplace(target = f, sensitivity = 100/n, dims = 1)
pparams1 <- DPParamsEps(epsilon = .1)
pparams2 <- DPParamsEps(epsilon = 1.25)
pparams3 <- DPParamsEps(epsilon = 3)

r1 <- releaseResponse(mechanism, privacyParams = pparams1, X = D)
r2 <- releaseResponse(mechanism, privacyParams = pparams2, X = D)
r3 <- releaseResponse(mechanism, privacyParams = pparams3, X = D)

#cat("Private response r$response:", r$response,
#"\nNon-private response f(D): ", f(D))

a = data.frame(
  id = 1:nrow(D),
  label = "PGA Score",
  pga = f(D),
  #far <- r1$response,
  #mid <- r2$response,
  #close <- r3$response,
  outcome = as.double(D$fatigue_m0)
  )

b = data.frame(
  id = 1:nrow(D),
  label = "DP PGA (epsilon=1.25)",
  #far <- r1$response,
  pga = r2$response,
  #close <- r3$response,
  outcome = as.double(D$fatigue_m0)
  )

c = data.frame(
  id = 1:nrow(D),
  label = "DP PGA (epsilon=0.8)",
  #far <- r1$response,
  pga = r1$response,
  #close <- r3$response,
  outcome = as.double(D$fatigue_m0)
  )

print(names(a))
print(names(b))
print(names(c))


VISDAT <- rbind.data.frame(a,b,c) 
#print(head(VISDAT))

#aes('Without DP',x=outcome, y=truth)

ggplot(VISDAT, aes(x=pga, y=outcome, color=label)) +
  geom_point() + 
  geom_smooth(method=lm) + 
  ylab("Fatigue Score") + xlab("Patient Global Assessment") + theme_bw()

print(paste0('Correlation between age and Fatigue m0',cor(x=a$pga, y=a$outcome)))
print(paste0('Correlation between age (DP with epsilon=1.25) and Fatigue m0',cor(x=b$pga, y=b$outcome)))
print(paste0('Correlation between age and fatigue (DP with epsilon=0.1)',cor(x=c$pga, y=c$outcome)))
#print(cor(x=c$age, y=c$outcome))

#VISDAT$outcome

```

## Differential Privacy in a Clinical Setting: Pros and Cons

## Pros: The Benefits of DP

-   Data can be shared more openly

-   Can now repeat **previous experiments** on a DP-version of research dataset

-   Easier approvals for **cross-organizational** and even **cross-border** data transfer from legal authorities...

-   Working **many-to-many**

## Pros: The Benefits of DP

-   Can be used to mitigate biases in decision making.

    -   If the original cohort were senior patients, results may not generalize to younger patients.

    -   With DP, we can mitigate selection-bias (over-representation) without having to resort to meta-analyses. One can think of it as a kind of regularization.

## **Cons: The Challenges of Differential Privacy in a Clinical Setting**

-   Not a lot of people know about DP.

    -   It has been used by technology companies for considerably longer

-   The perturbation of data can seem like a scary idea.

    -   It does mean sacrificing some accuracy, but as you have seen, results of analyses are often very realistic.

## **More about the long-term goals**

-   For now, our DP work is tangential to NORA/ScandRA.

![](img/SQUARE_logo.png)

![](img/SQUARE.png)

-   It is part of an ongoing Thesis Project at ITU Copenhagen.

    -   We are interested in examining the trade-offs related to different forms of differential privacy.

    -   How does DP impact the ability to perform accurate statistics?

    -   How can DP reduce selection bias due to cohort inclusion,

    -   or how can it introduce problems of over-generalization?

## **How you can help!**

-   More data means, noise needs to be less extreme.

    -   We can ensure as much privacy, without mutating the data to diverge so much.
    -   As usual, more data means more statistical power.

-   *Would you also be interested in participating?*

    -   Currently only data from Denmark.

    -   It would be nice if out partners from Sweden and Norway may also want to get involved :)

## Thank You!

Contact: ahmet\@zitelab.dk

\
